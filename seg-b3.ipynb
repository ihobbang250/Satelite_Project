{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision import models\nfrom datasets import load_metric\nfrom torchmetrics.functional import dice\nimport torch.optim as optim\nimport lightning as pl\nimport segmentation_models_pytorch as smp\n\nfrom transformers import SegformerForSemanticSegmentation\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import KFold\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"# RLE 디코딩 함수\ndef rle_decode(mask_rle, shape):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)\n\n#RLE 인코딩 함수\ndef rle_encode(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Dataset","metadata":{}},{"cell_type":"code","source":"class SatelliteDataset(Dataset):\n    def __init__(self, csv_file, image_dir, transform=None, infer=False):\n        self.data = pd.read_csv(csv_file)\n        self.image_dir = image_dir\n        self.transform = transform\n        self.infer = infer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_filename = self.data.iloc[idx, 1].lstrip('.')\n        img_path = self.image_dir + img_filename\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.infer:\n            if self.transform:\n                image = self.transform(image=image)['image']\n                \n            sample = {'pixel_values': image}\n            \n            return sample\n\n        mask_rle = self.data.iloc[idx, 2]\n        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n        \n        mask = torch.unsqueeze(mask, dim=0)\n    \n        sample = {'pixel_values': image, 'labels': mask}\n        \n        return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataLoader","metadata":{}},{"cell_type":"code","source":"transform = A.Compose(\n    [\n        A.RandomCrop(224, 224),\n        A.Normalize(),\n        A.Rotate(limit=60),\n        A.CoarseDropout(max_holes=8, max_height=0.25, max_width=0.25, min_holes=3, min_height=0.125, min_width=0.125, fill_value=0, mask_fill_value=0, p=1),\n        ToTensorV2()\n    ]\n\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = SatelliteDataset(csv_file='/kaggle/input/dacon-building-data/train.csv', image_dir='/kaggle/input/dacon-building-data', transform=transform)\n## 배치 사이즈 설정\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = SatelliteDataset(csv_file='/kaggle/input/dacon-building-data/test.csv', image_dir='/kaggle/input/dacon-building-data', transform=transform, infer=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model","metadata":{}},{"cell_type":"code","source":"class SegFormerModel(pl.LightningModule):\n    def __init__(self, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=10, model=None):\n        super(SegFormerModel, self).__init__()\n        #self.learningrate = learning_rate\n        self.metrics_interval = metrics_interval\n        self.train_dl = train_dataloader\n        self.val_dl = val_dataloader\n        self.test_dl = test_dataloader\n        self.model = model or get_initial_model()\n        #self.loss_module = smp.losses.SoftBCEWithLogitsLoss(weight=None, reduction=None, smooth_factor=None, pos_weight=None)\n        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=1.0, from_logits=True)\n        self.train_step_ious= []\n        self.validation_step_ious = []\n        self.validation_step_outputs = []\n        self.test_step_outputs = []\n        self.save_hyperparameters()\n        \n    def forward(self, images, masks=None):\n        outputs = self.model(pixel_values=images)\n        return outputs\n    \n    def training_step(self, batch, batch_idx):\n        masks = torch.squeeze(batch['labels']).long().to(device)\n        masks = masks.unsqueeze(1).to(device)\n        images = batch['pixel_values'].float().to(device)\n            \n        outputs = self.model(pixel_values=images, return_dict=True)\n        \n        upsampled_logits = nn.functional.interpolate(\n            outputs.logits, \n            size=masks.shape[-2:], \n            mode=\"bilinear\", \n            align_corners=False\n        ).contiguous().to(device)\n        \n    # predicted = upsampled_logits.argmax(dim=1)\n        loss = self.loss_module(upsampled_logits, masks)\n        tp, fp, fn, tn = smp.metrics.get_stats((upsampled_logits.sigmoid()>0.5).long(), masks.long(), mode='binary')\n        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        self.train_step_ious.append(iou)\n    \n        if batch_idx % self.metrics_interval == 0:\n            mean_iou = torch.stack(self.train_step_ious).mean()\n            # Log loss and metric\n            self.log('train_loss', loss)\n            self.log('train_mean_iou',  mean_iou)\n            \n            print(f\"Training loss: {loss:.5f}\")\n            print(\"\\n-----------------------\")\n\n        return {'loss': loss}\n    \n    \n#     def validation_step(self, batch, batch_idx):\n#         masks =  torch.squeeze(batch['labels']).long().to(device)\n#         masks = nn.functional.one_hot(masks, num_classes=self.num_classes).permute(0, 3, 1, 2).contiguous().to(device)\n#         images = batch['pixel_values'].float().to(device)\n        \n#         outputs = self.model(pixel_values=images, return_dict=True)\n        \n#         upsampled_logits = nn.functional.interpolate(\n#             outputs.logits, \n#             size=masks.shape[-2:], \n#             mode=\"bilinear\", \n#             align_corners=False\n#         ).contiguous()\n\n#         predicted = upsampled_logits.argmax(dim=1).to(device)\n#         loss = self.loss_module(upsampled_logits, masks)\n    \n#         tp, fp, fn, tn = smp.metrics.get_stats((upsampled_logits.sigmoid()>0.5).long(), masks.long(), mode='binary')\n#         iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        \n#         self.validation_step_ious.append(iou)\n#         self.validation_step_outputs.append(loss)\n        \n#         # Log loss and metric\n#         self.log('val_loss', loss)\n#         self.log(f\"IoU\", iou)\n        \n#         print(f\"Val Batch {batch_idx+1}: Metrics\")\n#         print(f\"-----------------------\\nStep Validation Loss: {loss:.5f}\")\n#         print(\"\\n-----------------------\")\n        \n#         return {'val_loss': loss, 'predicted': predicted}\n    \n    \n#     def on_validation_epoch_end(self):\n#         epoch_average_loss = torch.stack(self.validation_step_outputs).mean()\n#         val_step_mean_iou = torch.stack(self.validation_step_ious).mean()\n \n#         metrics = {\"val_loss\": epoch_average_loss, \"val_mean_iou\":val_step_mean_iou, }\n        \n#         print(f\"Val Epoch Metrics\")\n#         print(f\"Epoch IoU score: {val_step_mean_iou:.3f}\\n-----------------------\")    \n#         self.validation_step_outputs.clear()  # free memory\n#         return metrics\n    \n#     def test_step(self, batch, batch_idx):\n#         images = batch['pixel_values'].float().to(device)\n#         outputs = self.model(images, return_dict=True)\n        \n#         upsampled_logits = nn.functional.interpolate(\n#             outputs.logits, \n#             size=images.shape[-2:], \n#             mode=\"bilinear\", \n#             align_corners=False\n#         ).contiguous()\n        \n#         return upsampled_logits\n        \n        \n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        images = batch['pixel_values'].float()\n        return self.model(images, return_dict=True)\n        \n    def configure_optimizers(self):\n        ## lr 설정 필요\n        optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=1e-03, eps=1e-07)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max= 100, eta_min= 1e-06, last_epoch= -1)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}, \"monitor\": \"train_loss\"}\n    \n    def train_dataloader(self):\n        return self.train_dl\n    \n#     def val_dataloader(self):\n#         return self.val_dl\n    \n    def test_dataloader(self):\n        return self.test_dl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_initial_model():\n    return SegformerForSemanticSegmentation.from_pretrained(\n            \"nvidia/mit-b3\", \n            return_dict=True, \n            num_labels=1,\n            ignore_mismatched_sizes=True,\n            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\n# loss 모니터링 외부api wanb사이트에서 키 받아서 수정\nwandb_api = '0b6338174b96d25fc3dd9e12a4cf72eef795d891'\nwandb.login(key=wandb_api)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks import Callback\nfrom lightning.pytorch.callbacks import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import WandbLogger\nfrom lightning.pytorch.callbacks import LearningRateMonitor\n\n\n# 얼리스탑 여부 수정 부분\n# min_delta -> 최소 로스 변화량\n# patience -> 값이 k번 이내로 줄어야함\nearly_stop_callback = EarlyStopping(\n    monitor=\"train_loss\", \n    min_delta=0.01, \n    patience=3, \n    verbose=False, \n    mode=\"min\",\n)\n\ncheckpoint_callback = ModelCheckpoint(dirpath='/kaggle/working/checkpoint', save_top_k=1, monitor=\"train_loss\")\n\nwandb_logger = WandbLogger(project='seg1', log_model='all')\n\n# class FineTuneBatchSizeFinder(BatchSizeFinder):\n#     def __init__(self, milestones, *args, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.milestones = milestones\n\n#     def on_fit_start(self, *args, **kwargs):\n#         return\n\n#     def on_train_epoch_start(self, trainer, pl_module):\n#         if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n#             self.scale_batch_size(trainer, pl_module)\n            \n            \n# batch_size_callback = FineTuneBatchSizeFinder(milestones=(5, 10))\n\nlr_monitor_callback = LearningRateMonitor(logging_interval='step')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segformer = SegFormerModel( \n    train_dataloader = dataloader,\n    metrics_interval = 5\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback, checkpoint_callback, lr_monitor_callback],\n    max_epochs=6,\n    accelerator=\"gpu\",\n    devices = 1,\n    logger=wandb_logger\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(segformer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save & Load Model","metadata":{}},{"cell_type":"code","source":"checkpoint_callback.best_model_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_model = segformer.load_from_checkpoint(checkpoint_callback.best_model_path, map_location=torch.device('cpu'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"# load checkpoint\n#checkpoint_model = segformer.load_from_checkpoint('/kaggle/input/checkpoint/epoch0-step447 (1).ckpt', map_location=torch.device('cpu'))\n\ncheckpoint_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = trainer.predict(checkpoint_model, test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\n\nfor i,data in tqdm(enumerate(test_dataloader)):\n    image = data['pixel_values'].to('cpu')\n    upsampled_logits = nn.functional.interpolate(\n        outputs[i].logits, \n        size=image.shape[-2:], \n        mode=\"bilinear\", \n        align_corners=False\n    ).contiguous().to('cpu')\n    predicted = (torch.sigmoid(upsampled_logits) > 0.5).cpu().numpy()\n    predicted = np.squeeze(predicted, axis=1)\n    mask_rle = rle_encode(predicted)\n    if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n        result.append(-1)\n    else:\n        result.append(mask_rle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"submit = pd.read_csv('/kaggle/input/dacon-building-data/sample_submission.csv')\nsubmit['mask_rle'] = result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv('./submit.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}